# ğŸ”¬ Flaky Tests Comparative Study

**Estudo EmpÃ­rico Comparativo de DetecÃ§Ã£o de Testes Flaky em Projetos Open-Source**

> Projeto da disciplina IF1009 - Testes e ValidaÃ§Ã£o de Software  
> Centro de InformÃ¡tica - UFPE | 2025.1

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Java](https://img.shields.io/badge/Java-8%2F11+-orange.svg)](https://www.java.com/)
[![License](https://img.shields.io/badge/license-Academic-green.svg)](LICENSE)

---

## ğŸ“š Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)- [Quick Start](#-quick-start)- [Setup Inicial](#-setup-inicial)
- [Comandos Makefile](#-comandos-makefile)
- [DocumentaÃ§Ã£o Completa](#-documentaÃ§Ã£o-completa)
- [Principais ConclusÃµes](#-principais-conclusÃµes)
- [FAQ e Troubleshooting](#-faq-e-troubleshooting)
- [Requisitos](#-requisitos)
- [ReferÃªncias](#-referÃªncias)
- [Equipe](#-equipe)

## ğŸ“‹ VisÃ£o Geral

## ï¿½ğŸ“‹ VisÃ£o Geral
Este projeto conduz um **estudo empÃ­rico comparativo** sobre detecÃ§Ã£o de testes flaky em 8 projetos open-source, utilizando anÃ¡lise estatÃ­stica rigorosa.

### ğŸ¯ Objetivo

Avaliar e comparar a eficÃ¡cia de duas ferramentas de detecÃ§Ã£o de testes flaky (NonDex para Java e pytest-rerun para Python) atravÃ©s de mÃ©tricas estatÃ­sticas robustas, identificando padrÃµes e caracterÃ­sticas dos testes flaky em diferentes ecossistemas de linguagens.

### âœ¨ CaracterÃ­sticas

- ğŸ“Š **AnÃ¡lise EstatÃ­stica Rigorosa**: Wilson Score CI (95%), teste binomial com Î± = 0.05
- ğŸ”„ **ExecuÃ§Ãµes MÃºltiplas**: 20 rodadas para projetos Python, detecÃ§Ã£o automatizada para Java
- ğŸ“ˆ **Dashboard Interativo**: VisualizaÃ§Ã£o em tempo real com Streamlit (3 modos de escala)
- ğŸ“‹ **RelatÃ³rios Completos**: CSV, JSON, Markdown e HTML exportÃ¡veis
- ğŸ¨ **MÃ©tricas AvanÃ§adas**: Taxa de falha, p-value, severidade, evoluÃ§Ã£o temporal
- ğŸ”§ **AutomaÃ§Ã£o Total**: Makefile com comandos simples para todas as operaÃ§Ãµes

### Ferramentas de DetecÃ§Ã£o
- **NonDex** (Java): Detecta nÃ£o-determinismo atravÃ©s de mÃºltiplas execuÃ§Ãµes com diferentes ordenaÃ§Ãµes de coleÃ§Ãµes e APIs
- **pytest-rerun** (Python): Detecta testes flaky atravÃ©s de 20 execuÃ§Ãµes repetidas com anÃ¡lise estatÃ­stica (p-value, IC 95%)

### Projetos Analisados

**Java (4 projetos - Apache Commons):**
- `commons-lang` (57,778 testes) - Utilidades para Java
- `commons-codec` (18,549 testes) - Codificadores/decodificadores
- `commons-collections` (8,428 testes) - Estruturas de dados
- `guava` - Core libraries do Google

**Python (4 projetos):**
- `httpie` - Cliente HTTP moderno
- `flask` - Framework web
- `black` - Formatador de cÃ³digo
- `httpx` - Cliente HTTP assÃ­ncrono

### MÃ©tricas EstatÃ­sticas

O sistema calcula **mÃ©tricas rigorosas** para cada teste:
- Taxa de falha e intervalos de confianÃ§a (95%)
- P-value e significÃ¢ncia estatÃ­stica (Î± = 0.05)
- ClassificaÃ§Ã£o de severidade (low/medium/high)
- VariÃ¢ncia e desvio padrÃ£o

**Resultados atuais**: 680 testes flaky detectados em 3/8 projetos, com taxa variando de 0.07% a 1.16%.

---

## ğŸš€ Quick Start

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/PedroCLins/flaky-tests-comparative-study.git
cd flaky-tests-comparative-study

# 2. Configure o ambiente
make setup

# 3. Execute todos os testes e visualize
make all

# 4. Abra o dashboard interativo
make dashboard
```

O dashboard estarÃ¡ disponÃ­vel em `http://localhost:8501`

---

## ğŸš€ Setup Inicial

### 1. Clone este repositÃ³rio

```bash
git clone https://github.com/PedroCLins/flaky-tests-comparative-study.git
cd flaky-tests-comparative-study
```

### 2. Crie o diretÃ³rio de experimentos

Crie um diretÃ³rio separado para os projetos que serÃ£o testados:

```bash
cd ..
mkdir flaky-tests-experiments
cd flaky-tests-experiments
```

### 3. Clone os projetos a serem analisados

Clone os repositÃ³rios dos projetos que vocÃª deseja analisar:

**Projetos Java (Apache Commons):**
```bash
git clone https://github.com/apache/commons-lang.git
git clone https://github.com/apache/commons-codec.git
git clone https://github.com/apache/commons-collections.git
git clone https://github.com/google/guava.git
```

**Projetos Python:**
```bash
git clone https://github.com/httpie/cli.git httpie
git clone https://github.com/pallets/flask.git
git clone https://github.com/psf/black.git
git clone https://github.com/encode/httpx.git
```

Estrutura esperada:
```
.
â”œâ”€â”€ flaky-tests-comparative-study/    # Este repositÃ³rio
â””â”€â”€ flaky-tests-experiments/          # Projetos a serem analisados
    â”œâ”€â”€ commons-lang/
    â”œâ”€â”€ commons-codec/
    â”œâ”€â”€ commons-collections/
    â”œâ”€â”€ guava/
    â”œâ”€â”€ httpie/
    â”œâ”€â”€ flask/
    â”œâ”€â”€ black/
    â””â”€â”€ httpx/
```

### 4. Configure o arquivo `.env`

Volte para o diretÃ³rio do projeto e crie um arquivo `.env`:

```bash
cd ../flaky-tests-comparative-study
```

Crie o arquivo `.env` com o seguinte conteÃºdo:

```bash
# Directory paths
EXPERIMENT_DIR=../flaky-tests-experiments
SCRIPTS_DIR=./scripts
RESULTS_DIR=./results

# Project names
JAVA_PROJECTS=commons-lang commons-codec commons-collections guava
PYTHON_PROJECTS=httpie flask black httpx

# Test configuration
PYTEST_ROUNDS=20  # NÃºmero de execuÃ§Ãµes para anÃ¡lise estatÃ­stica
```

**Nota:** O nÃºmero de rodadas (20) foi escolhido para fornecer poder estatÃ­stico adequado (>90%) para detectar testes flaky com taxa de falha â‰¥ 10%.

### 5. Instale as dependÃªncias

Execute o setup para instalar todas as dependÃªncias necessÃ¡rias:

```bash
make setup
```

Isso irÃ¡:
- Verificar instalaÃ§Ã£o de Java e Maven
- Criar um ambiente virtual Python (`.venv`)
- Instalar pacotes pytest e visualizaÃ§Ã£o necessÃ¡rios

## ğŸƒ Executando os Testes

### Executar tudo

Para executar todas as ferramentas em todos os projetos:

```bash
make all
```

### Executar ferramentas especÃ­ficas

**NonDex (apenas projetos Java):**
```bash
make nondex
```

**pytest-rerun (apenas projetos Python):**
```bash
make python
```

### ExecuÃ§Ã£o em Background

Para processos longos, recomenda-se usar `tmux`:

```bash
# Cria uma sessÃ£o tmux
tmux new -s flaky-tests

# Execute os testes
make all

# Desanexar: Pressione Ctrl+b, depois d
# Reanexar depois: tmux attach -t flaky-tests
```

Ou execute em background simples:

```bash
nohup make all > output.log 2>&1 &
tail -f output.log
```

Veja o arquivo [RUN_GUIDE.md](RUN_GUIDE.md) para mais opÃ§Ãµes de execuÃ§Ã£o em background.

## ğŸ“Š Resultados

Os resultados sÃ£o salvos em `results/` organizados por projeto e ferramenta:

```
results/
â”œâ”€â”€ commons-lang/
â”‚   â””â”€â”€ nondex/
â”‚       â””â”€â”€ 2025-12-10_16-21-10/
â”‚           â”œâ”€â”€ commit.txt
â”‚           â”œâ”€â”€ nondex.log          # Log completo do NonDex
â”‚           â”œâ”€â”€ summary.txt         # Resumo de erros e warnings
â”‚           â””â”€â”€ metadata.json       # Metadados da execuÃ§Ã£o
â”œâ”€â”€ httpie/
â”‚   â””â”€â”€ pytest-rerun/
â”‚       â””â”€â”€ 2025-12-10_16-21-10/
â”‚           â”œâ”€â”€ commit.txt
â”‚           â”œâ”€â”€ runs.csv            # Dados por rodada (para mÃ©tricas)
â”‚           â”œâ”€â”€ summary.txt         # Resumo de testes flaky
â”‚           â”œâ”€â”€ run_1.log
â”‚           â”œâ”€â”€ run_2.log
â”‚           â”œâ”€â”€ ...
â”‚           â”œâ”€â”€ run_20.log
â”‚           â””â”€â”€ metadata.json
â””â”€â”€ ...
```

### VisualizaÃ§Ã£o e AnÃ¡lise

ApÃ³s executar os testes, analise os resultados:

```bash
# Gerar relatÃ³rios e CSVs com mÃ©tricas
make visualize

# Abrir dashboard interativo (Streamlit)
make dashboard
```

#### ğŸ¨ Dashboard Interativo

O dashboard Streamlit oferece uma interface completa para anÃ¡lise:

**VisÃ£o Geral**
- ğŸ“Š MÃ©tricas agregadas por projeto (total de testes, flaky detectados, taxa %)
- ğŸ“ˆ GrÃ¡ficos comparativos com 3 modos de visualizaÃ§Ã£o:
  - **Separado por Projeto**: GrÃ¡ficos individuais para melhor anÃ¡lise
  - **Escala LogarÃ­tmica**: VisualizaÃ§Ã£o simultÃ¢nea de projetos com escalas muito diferentes
  - **Todos Juntos**: ComparaÃ§Ã£o direta em escala linear
- ğŸ¯ Indicadores visuais de severidade (baixo/mÃ©dio/alto)

**AnÃ¡lise Detalhada Python (pytest-rerun)**
- ğŸ”¬ MÃ©tricas individuais por teste com estatÃ­sticas completas
- ğŸ“‰ Taxa de falha com Intervalo de ConfianÃ§a Wilson (95%)
- ğŸ§ª P-valor e significÃ¢ncia estatÃ­stica (Î± = 0.05)
- ğŸ“Š GrÃ¡ficos de distribuiÃ§Ã£o de falhas por rodada
- â° EvoluÃ§Ã£o temporal das detecÃ§Ãµes

**AnÃ¡lise Java (NonDex)**
- âš ï¸ Lista de testes com nÃ£o-determinismo detectado
- ğŸ“‹ Logs completos de warnings do NonDex
- ğŸ“Š Contagem agregada por projeto

**ExportaÃ§Ã£o**
- ğŸ“‹ Tabelas filtrÃ¡veis e ordenÃ¡veis interativamente
- ğŸ’¾ Download em CSV/JSON para anÃ¡lise posterior
- ğŸ“„ RelatÃ³rio HTML completo com todos os dados

**RelatÃ³rios gerados** em `visualization/reports/`:
- `project_summary.csv` - MÃ©tricas agregadas por projeto
- `test_metrics_detailed.csv` - MÃ©tricas individuais de cada teste
- `flaky_tests_metrics.csv` - Apenas testes com p < 0.05
- `summary_report.md` - RelatÃ³rio em markdown

### Exemplo de Resultados Reais

Baseado nas execuÃ§Ãµes realizadas:

| Projeto | Total Testes | Flaky Detectados | Taxa | Severidade |
|---------|--------------|------------------|------|------------|
| commons-lang | 57,778 | 673 | 1.16% | CrÃ­tico |
| commons-collections | 8,428 | 6 | 0.07% | Baixo |
| commons-codec | 18,549 | 0 | 0% | - |
| httpie | 3 | 1 | 33% | Baixo (5% falha) |
| flask | 1 | 0 | 0% | - |

## ğŸ› ï¸ Estrutura do Projeto

```
.
â”œâ”€â”€ Makefile                      # Comandos principais
â”œâ”€â”€ README.md                     # Este arquivo
â”œâ”€â”€ RUN_GUIDE.md                  # Guia de execuÃ§Ã£o em background
â”œâ”€â”€ METRICS.md                    # DocumentaÃ§Ã£o das mÃ©tricas
â”œâ”€â”€ RELATORIO_METRICAS.md         # RelatÃ³rio acadÃªmico completo
â”œâ”€â”€ .env                          # ConfiguraÃ§Ã£o (criar manualmente)
â”œâ”€â”€ .gitignore                    # Arquivos ignorados
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_dependencies.sh     # Setup de dependÃªncias
â”‚   â”œâ”€â”€ run_nondex.sh             # Script NonDex
â”‚   â”œâ”€â”€ run_py_flaky_detection.sh # Script pytest (20 rodadas)
â”‚   â”œâ”€â”€ run_visualization.sh      # Script de visualizaÃ§Ã£o
â”‚   â”œâ”€â”€ cleanup.sh                # Limpeza de resultados
â”‚   â””â”€â”€ monitor_tests.sh          # Monitoramento de execuÃ§Ã£o
â”‚
â”œâ”€â”€ visualization/
â”‚   â”œâ”€â”€ analyze_results.py        # Analisador principal
â”‚   â”œâ”€â”€ dashboard.py              # Dashboard Streamlit
â”‚   â”œâ”€â”€ metrics.py                # CÃ¡lculo de mÃ©tricas estatÃ­sticas
â”‚   â”œâ”€â”€ html_report.py            # Gerador de relatÃ³rios HTML
â”‚   â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”‚   â”œâ”€â”€ activate.sh               # Script de ativaÃ§Ã£o do venv
â”‚   â””â”€â”€ reports/                  # RelatÃ³rios gerados
â”‚       â”œâ”€â”€ project_summary.csv
â”‚       â”œâ”€â”€ test_metrics_detailed.csv
â”‚       â”œâ”€â”€ flaky_tests_metrics.csv
â”‚       â””â”€â”€ summary_report.md
â”‚
â””â”€â”€ results/                      # Resultados dos experimentos
    â”œâ”€â”€ commons-lang/nondex/
    â”œâ”€â”€ commons-collections/nondex/
    â”œâ”€â”€ httpie/pytest-rerun/
    â””â”€â”€ ...
```

## ğŸ“ Comandos Makefile

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `make all` | Executa setup + todos os testes + visualizaÃ§Ã£o |
| `make setup` | Instala dependÃªncias (Java, Maven, Python, venv) |
| `make java` | Executa NonDex em todos os projetos Java |
| `make nondex` | Alias para `make java` |
| `make python` | Executa pytest-rerun (20 rodadas) em projetos Python |
| `make visualize` | Gera relatÃ³rios CSV, JSON e markdown |
| `make dashboard` | Abre dashboard interativo Streamlit (porta 8501) |
| `make clean` | Remove resultados e arquivos temporÃ¡rios |
| `make help` | Lista todos os comandos disponÃ­veis |

### Comandos AvanÃ§ados

```bash
# Executar apenas um projeto especÃ­fico
RUN_PROJECT=commons-lang make nondex

# Alterar nÃºmero de rodadas pytest (padrÃ£o: 20)
PYTEST_ROUNDS=50 make python

# Executar em background com tmux
tmux new -s flaky-tests
make all
# Ctrl+b, d para desanexar

# Monitorar progresso
tail -f results/*/*/summary.txt
```

## ğŸ“– DocumentaÃ§Ã£o Completa

Para informaÃ§Ãµes detalhadas sobre as mÃ©tricas utilizadas e metodologia estatÃ­stica:

- **[METRICS.md](METRICS.md)** - DocumentaÃ§Ã£o tÃ©cnica das mÃ©tricas implementadas
- **[RELATORIO_METRICAS.md](RELATORIO_METRICAS.md)** - RelatÃ³rio acadÃªmico completo com fundamentaÃ§Ã£o estatÃ­stica
- **[RUN_GUIDE.md](RUN_GUIDE.md)** - Guia detalhado de execuÃ§Ã£o e monitoramento
- **[TEST_STATUS.md](TEST_STATUS.md)** - Status atual dos testes por projeto
- **[FINAL_STATUS.md](FINAL_STATUS.md)** - Status final da execuÃ§Ã£o experimental

### MÃ©tricas EstatÃ­sticas Implementadas

**Para projetos Python (pytest-rerun):**
- Taxa de falha individual por teste
- Intervalo de ConfianÃ§a Wilson Score (95%)
- P-valor (teste binomial, Î± = 0.05)
- ClassificaÃ§Ã£o de severidade baseada na taxa de falha
- EvoluÃ§Ã£o temporal das detecÃ§Ãµes

**Para projetos Java (NonDex):**
- DetecÃ§Ã£o agregada de nÃ£o-determinismo
- Contagem de testes flaky detectados
- Logs completos de warnings e erros

## ğŸ¯ Principais ConclusÃµes

Baseado em 24 execuÃ§Ãµes completas (3 rodadas Ã— 8 projetos):

1. **DetecÃ§Ã£o Eficaz**: 680 testes flaky identificados em 3 dos 8 projetos analisados
2. **Projeto Mais CrÃ­tico**: commons-lang com 673 testes flaky (1.16% da base de testes)
3. **Ferramentas Complementares**: NonDex e pytest-rerun detectam tipos diferentes de flakiness
4. **DistribuiÃ§Ã£o Desigual**: ConcentraÃ§Ã£o de problemas em poucos projetos indica fatores especÃ­ficos

## â“ FAQ e Troubleshooting

<details>
<summary><b>Por que 20 rodadas para pytest?</b></summary>

O nÃºmero de 20 rodadas foi escolhido para fornecer **poder estatÃ­stico > 90%** para detectar testes com taxa de falha â‰¥ 10%, conforme anÃ¡lise estatÃ­stica detalhada em [RELATORIO_METRICAS.md](RELATORIO_METRICAS.md).
</details>

<details>
<summary><b>Como interpretar o p-value?</b></summary>

Um p-value < 0.05 indica que a probabilidade de observar aquele padrÃ£o de falhas por acaso Ã© menor que 5%, sugerindo **forte evidÃªncia estatÃ­stica** de flakiness. Veja explicaÃ§Ã£o completa em [METRICS.md](METRICS.md).
</details>

<details>
<summary><b>Por que alguns projetos nÃ£o tÃªm mÃ©tricas detalhadas?</b></summary>

- **Projetos Java (NonDex)**: Fornecem apenas detecÃ§Ã£o agregada, sem dados por rodada individual
- **Projetos Python (pytest-rerun)**: Fornecem dados de todas as 20 rodadas, permitindo cÃ¡lculo completo de mÃ©tricas estatÃ­sticas
</details>

<details>
<summary><b>Os testes estÃ£o travados/muito lentos</b></summary>

Alguns projetos Java (especialmente commons-lang com 57k testes) podem levar vÃ¡rias horas. Use:
```bash
# Monitorar progresso
tail -f results/*/*/summary.txt

# Executar em background com tmux
tmux new -s flaky-tests
make all
# Ctrl+b, d para desanexar
```
</details>

<details>
<summary><b>Como limpar resultados antigos?</b></summary>

```bash
make clean  # Remove todos os resultados e temporÃ¡rios
```
</details>

## ğŸ”§ Requisitos

- **Java**: JDK 8 ou 11+
- **Maven**: 3.6+
- **Python**: 3.8+
- **Git**: Para clonar repositÃ³rios

## ğŸ“š ReferÃªncias

### Ferramentas de DetecÃ§Ã£o
- **NonDex** - [GitHub](https://github.com/TestingResearchIllinois/NonDex) | Bell, J., et al. "DeFlaker: Automatically Detecting Flaky Tests." ICSE 2018.
- **pytest-rerunfailures** - [GitHub](https://github.com/pytest-dev/pytest-rerunfailures) | Plugin para re-execuÃ§Ã£o de testes pytest

### Projetos Analisados
- **Apache Commons Lang** - [GitHub](https://github.com/apache/commons-lang)
- **Apache Commons Collections** - [GitHub](https://github.com/apache/commons-collections)
- **Apache Commons Codec** - [GitHub](https://github.com/apache/commons-codec)
- **Google Guava** - [GitHub](https://github.com/google/guava)
- **HTTPie** - [GitHub](https://github.com/httpie/httpie)
- **Flask** - [GitHub](https://github.com/pallets/flask)
- **Black** - [GitHub](https://github.com/psf/black)
- **HTTPX** - [GitHub](https://github.com/encode/httpx)

### Literatura Relacionada
- Luo, Q., et al. "An Empirical Analysis of Flaky Tests." FSE 2014.
- Thorve, S., et al. "An Empirical Study of Flaky Tests in Android Apps." ICSME 2018.
- Eck, M., et al. "Understanding Flaky Tests: The Developer's Perspective." ESEC/FSE 2019.

## ğŸ‘¥ Equipe

**CIn - UFPE | Testes e ValidaÃ§Ã£o de Software**

- Ãriston AragÃ£o \<aaa10@cin.ufpe.br>
- Fabriely Luana \<flps@cin.ufpe.br>
- Pedro Campelo \<pcl@cin.ufpe.br>

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins acadÃªmicos como parte da disciplina de Testes e ValidaÃ§Ã£o de Software do Centro de InformÃ¡tica da UFPE.

Os projetos analisados mantÃªm suas respectivas licenÃ§as originais (Apache 2.0, MIT, etc.).
